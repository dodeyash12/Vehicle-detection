{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94602c57-2b49-431c-ac9c-ff90782b37e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240b2637-36b9-4e39-9def-c9357d8aadbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of vehicle images: 8968\n",
      "Number of non-vehicle images: 8792\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_folder(folder_path):\n",
    "    # Get all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    # Count image files (you can specify extensions like .jpg, .png, etc.)\n",
    "    image_count = sum(1 for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')))\n",
    "    return image_count\n",
    "\n",
    "vehicle_dir = 'data4/non-vehicles'\n",
    "non_vehicle_dir = 'data4/vehicles'\n",
    "# Count images in each folder\n",
    "vehicle_count = count_images_in_folder(vehicle_dir)\n",
    "non_vehicle_count = count_images_in_folder(non_vehicle_dir)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of vehicle images: {vehicle_count}\")\n",
    "print(f\"Number of non-vehicle images: {non_vehicle_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "787b57ef-a850-42ae-ba41-f58ae58ce7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 images: 1000 vehicles and 1000 non-vehicles\n",
      "Accuracy: 0.96\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       201\n",
      "           1       0.94      0.98      0.96       199\n",
      "\n",
      "    accuracy                           0.96       400\n",
      "   macro avg       0.96      0.96      0.96       400\n",
      "weighted avg       0.96      0.96      0.96       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from glob import glob\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load and Limit Dataset\n",
    "def load_limited_dataset(vehicle_dir, non_vehicle_dir, limit=75):\n",
    "    # Load all vehicle and non-vehicle images\n",
    "    vehicle_images = glob(vehicle_dir + '/*.png')\n",
    "    non_vehicle_images = glob(non_vehicle_dir + '/*.png')\n",
    "    \n",
    "    # Randomly sample the specified limit from each category\n",
    "    vehicle_images = sample(vehicle_images, min(limit, len(vehicle_images)))\n",
    "    non_vehicle_images = sample(non_vehicle_images, min(limit, len(non_vehicle_images)))\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load sampled vehicle images\n",
    "    for file in vehicle_images:\n",
    "        images.append(imread(file))\n",
    "        labels.append(1)  # Label for vehicles\n",
    "    \n",
    "    # Load sampled non-vehicle images\n",
    "    for file in non_vehicle_images:\n",
    "        images.append(imread(file))\n",
    "        labels.append(0)  # Label for non-vehicles\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Paths to your dataset folders\n",
    "vehicle_dir = 'data4/non-vehicles'\n",
    "non_vehicle_dir = 'data4/vehicles'\n",
    "\n",
    "# Load limited dataset (1000 vehicles and 1000 non-vehicles)\n",
    "images, labels = load_limited_dataset(vehicle_dir, non_vehicle_dir, limit=1000)\n",
    "\n",
    "print(f\"Loaded {len(images)} images: {sum(labels)} vehicles and {len(labels) - sum(labels)} non-vehicles\")\n",
    "\n",
    "# Step 2: Feature Extraction\n",
    "def extract_hog_features(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2)):\n",
    "    gray_image = rgb2gray(image)  # Convert to grayscale\n",
    "    features = hog(\n",
    "        gray_image,\n",
    "        orientations=orientations,\n",
    "        pixels_per_cell=pixels_per_cell,\n",
    "        cells_per_block=cells_per_block,\n",
    "        block_norm='L2-Hys',\n",
    "        visualize=False\n",
    "    )\n",
    "    return features\n",
    "\n",
    "def extract_features(images):\n",
    "    feature_list = [extract_hog_features(image) for image in images]\n",
    "    return np.array(feature_list)\n",
    "\n",
    "# Extract features for all images\n",
    "features = extract_features(images)\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the SVM Classifier\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "y_pred = svm.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb16861-f713-4f67-b286-01daa1023740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093c2a8-90aa-4de6-a380-84de371defc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac2f3ec4-5400-420c-aa3e-58bfd4930269",
   "metadata": {},
   "source": [
    "# HOG AND SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d426f3e9-047b-46bf-b304-2f29f1138291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.75%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Vehicle       0.95      0.96      0.96      1801\n",
      "     Vehicle       0.96      0.95      0.96      1751\n",
      "\n",
      "    accuracy                           0.96      3552\n",
      "   macro avg       0.96      0.96      0.96      3552\n",
      "weighted avg       0.96      0.96      0.96      3552\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vehicle_detection_svm.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Function to load all images from a folder\n",
    "def load_images_from_folder(folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if filepath.endswith(('.png')):\n",
    "            img = cv2.imread(filepath)\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                img = cv2.resize(img, (64, 64))  # Resize for uniformity\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Paths to your vehicle and non-vehicle folders\n",
    "vehicle_folder = 'data4/vehicles'\n",
    "no_vehicle_folder = \"data4/non-vehicles\"\n",
    "\n",
    "# Load all images from both folders\n",
    "vehicle_images, vehicle_labels = load_images_from_folder(vehicle_folder, 1)\n",
    "no_vehicle_images, no_vehicle_labels = load_images_from_folder(no_vehicle_folder, 0)\n",
    "\n",
    "# Combine the data\n",
    "X = np.array(vehicle_images + no_vehicle_images)\n",
    "y = np.array(vehicle_labels + no_vehicle_labels)\n",
    "\n",
    "# Function to extract HOG features\n",
    "def extract_hog_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        feature = hog(img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                      cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# Extract HOG features for the dataset\n",
    "X_hog = extract_hog_features(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hog, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm = LinearSVC(max_iter=10000)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"No Vehicle\", \"Vehicle\"]))\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(svm, \"vehicle_detection_svm.pkl\")\n",
    "\n",
    "# Function to predict whether an image contains a vehicle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0393fe1-285e-44f4-8cb4-6702d3245d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for data4/full.png: No Vehicle\n"
     ]
    }
   ],
   "source": [
    "def predict_vehicle(image_path, svm):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (64, 64))\n",
    "    hog_features = hog(resized, orientations=9, pixels_per_cell=(8, 8), \n",
    "                       cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
    "    prediction = svm.predict([hog_features])\n",
    "    return \"Vehicle\" if prediction[0] == 1 else \"No Vehicle\"\n",
    "\n",
    "# Example prediction with a new image\n",
    "test_image_path = \"data4/full.png\"\n",
    "print(f\"Prediction for {test_image_path}: {predict_vehicle(test_image_path, svm)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec82ced-08dd-4c56-8b1d-1a40b776d361",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6808701-3b3e-40da-900b-6bbf991e845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Function to load all images from a folder\n",
    "def load_images_from_folder(folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if filepath.endswith(('.png')):\n",
    "            img = cv2.imread(filepath)\n",
    "            if img is not None:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                img = cv2.resize(img, (64, 64))  # Resize to uniform size\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Paths to your vehicle and non-vehicle folders\n",
    "vehicle_folder = 'data4/vehicles'\n",
    "no_vehicle_folder = \"data4/non-vehicles\"\n",
    "\n",
    "# Load all images from both folders\n",
    "vehicle_images, vehicle_labels = load_images_from_folder(vehicle_folder, 1)\n",
    "no_vehicle_images, no_vehicle_labels = load_images_from_folder(no_vehicle_folder, 0)\n",
    "\n",
    "# Combine the data\n",
    "X = np.array(vehicle_images + no_vehicle_images)\n",
    "y = np.array(vehicle_labels + no_vehicle_labels)\n",
    "\n",
    "# Function to extract HOG features\n",
    "def extract_hog_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        feature = hog(img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                      cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# Extract HOG features for the dataset\n",
    "X_hog = extract_hog_features(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hog, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "680971c2-5dcb-46a4-ac95-de28fe7f8d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 90.43%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Vehicle       0.90      0.91      0.91      1801\n",
      "     Vehicle       0.91      0.90      0.90      1751\n",
      "\n",
      "    accuracy                           0.90      3552\n",
      "   macro avg       0.90      0.90      0.90      3552\n",
      "weighted avg       0.90      0.90      0.90      3552\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vehicle_detection_tree.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a Decision Tree classifier\n",
    "tree_model = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(f\"Decision Tree Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"No Vehicle\", \"Vehicle\"]))\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(tree_model, \"vehicle_detection_tree.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17df1153-5df9-465f-8bfe-4ef5a292fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for data4/full.png: No Vehicle\n"
     ]
    }
   ],
   "source": [
    "# Function to predict whether an image contains a vehicle\n",
    "def predict_vehicle(image_path, tree_model):\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (64, 64))\n",
    "    hog_features = hog(resized, orientations=9, pixels_per_cell=(8, 8), \n",
    "                       cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
    "    prediction = tree_model.predict([hog_features])\n",
    "    return \"Vehicle\" if prediction[0] == 1 else \"No Vehicle\"\n",
    "\n",
    "# Example prediction with a new image\n",
    "test_image_path = \"data4/full.png\"\n",
    "print(f\"Prediction for {test_image_path}: {predict_vehicle(test_image_path, tree_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80c6f5ca-0702-4db5-83b0-66e9a95d5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "data4/non-vehicles/image995.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0f2614e-4130-4cd9-9c62-bead8a4b2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, label, img_size=(64, 64)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if filepath.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            img = cv2.imread(filepath)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, img_size)  # Resize for uniformity\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Paths to vehicle and non-vehicle folders\n",
    "vehicle_folder = 'data4/vehicles'\n",
    "no_vehicle_folder = \"data4/non-vehicles\"\n",
    "\n",
    "# Load vehicle and non-vehicle data\n",
    "vehicle_images, vehicle_labels = load_images_from_folder(vehicle_folder, 1)\n",
    "no_vehicle_images, no_vehicle_labels = load_images_from_folder(no_vehicle_folder, 0)\n",
    "\n",
    "# Combine and preprocess the dataset\n",
    "X = np.array(vehicle_images + no_vehicle_images) / 255.0  # Normalize pixel values to [0, 1]\n",
    "y = np.array(vehicle_labels + no_vehicle_labels)\n",
    "\n",
    "# Reshape data for CNN (ensure images are in the format (height, width, channels))\n",
    "X = X.reshape(-1, 64, 64, 3)\n",
    "\n",
    "# Convert labels to categorical for binary classification\n",
    "y = to_categorical(y, num_classes=2)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37cba322-eb60-4957-b5c8-9927cdbfd80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashdode/Library/Python/3.12/lib/python/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8799 - loss: 0.2724 - val_accuracy: 0.9792 - val_loss: 0.0656\n",
      "Epoch 2/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9807 - loss: 0.0608 - val_accuracy: 0.9783 - val_loss: 0.0563\n",
      "Epoch 3/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9862 - loss: 0.0385 - val_accuracy: 0.9904 - val_loss: 0.0237\n",
      "Epoch 4/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9911 - loss: 0.0277 - val_accuracy: 0.9887 - val_loss: 0.0313\n",
      "Epoch 5/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9933 - loss: 0.0212 - val_accuracy: 0.9947 - val_loss: 0.0160\n",
      "Epoch 6/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9953 - loss: 0.0135 - val_accuracy: 0.9851 - val_loss: 0.0368\n",
      "Epoch 7/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.9901 - loss: 0.0257 - val_accuracy: 0.9921 - val_loss: 0.0194\n",
      "Epoch 8/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.9942 - loss: 0.0175 - val_accuracy: 0.9873 - val_loss: 0.0335\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9927 - loss: 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.47%\n"
     ]
    }
   ],
   "source": [
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    # Convolutional layer 1\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten the output of convolutional layers\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout for regularization\n",
    "    Dense(2, activation='softmax')  # Output layer with 2 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"cnn_vehicle_detection.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b98b0cbf-ec38-434e-b62c-8522494439d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Prediction for data4/full.png: No Vehicle\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained CNN model\n",
    "model = load_model(\"cnn_vehicle_detection.h5\")\n",
    "\n",
    "def predict_vehicle(image_path, model):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (64, 64)) / 255.0  # Resize and normalize\n",
    "    img = img.reshape(1, 64, 64, 3)  # Add batch dimension\n",
    "    prediction = model.predict(img)\n",
    "    return \"Vehicle\" if np.argmax(prediction) == 1 else \"No Vehicle\"\n",
    "\n",
    "# Example prediction\n",
    "test_image_path = \"data4/full.png\"\n",
    "print(f\"Prediction for {test_image_path}: {predict_vehicle(test_image_path, model)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e74890-fcd7-4c06-b83e-229e66781b1b",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c43bb1-2b8e-4a57-a24b-cd649d106060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MobileNetV2 for Transfer Learning...\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yashdode/Library/Python/3.12/lib/python/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 60ms/step - accuracy: 0.9258 - loss: 0.1856 - val_accuracy: 0.9868 - val_loss: 0.0364\n",
      "Epoch 2/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 62ms/step - accuracy: 0.9739 - loss: 0.0718 - val_accuracy: 0.9885 - val_loss: 0.0335\n",
      "Epoch 3/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 67ms/step - accuracy: 0.9732 - loss: 0.0675 - val_accuracy: 0.9876 - val_loss: 0.0299\n",
      "Epoch 4/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 71ms/step - accuracy: 0.9777 - loss: 0.0588 - val_accuracy: 0.9921 - val_loss: 0.0246\n",
      "Epoch 5/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 76ms/step - accuracy: 0.9789 - loss: 0.0603 - val_accuracy: 0.9966 - val_loss: 0.0132\n",
      "Epoch 6/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 77ms/step - accuracy: 0.9799 - loss: 0.0520 - val_accuracy: 0.9947 - val_loss: 0.0178\n",
      "Epoch 7/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.9853 - loss: 0.0442 - val_accuracy: 0.9913 - val_loss: 0.0287\n",
      "Epoch 8/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 80ms/step - accuracy: 0.9831 - loss: 0.0455 - val_accuracy: 0.9952 - val_loss: 0.0131\n",
      "Epoch 9/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 79ms/step - accuracy: 0.9855 - loss: 0.0430 - val_accuracy: 0.9949 - val_loss: 0.0153\n",
      "Epoch 10/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 75ms/step - accuracy: 0.9845 - loss: 0.0459 - val_accuracy: 0.9910 - val_loss: 0.0259\n",
      "Epoch 11/20\n",
      "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 77ms/step - accuracy: 0.9843 - loss: 0.0427 - val_accuracy: 0.9913 - val_loss: 0.0270\n",
      "\u001b[1m111/111\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.9933 - loss: 0.0181\n",
      "Test Accuracy: 99.52%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Function to load images from a folder\n",
    "def load_images_from_folder(folder, label, img_size=(128, 128)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if filepath.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            img = cv2.imread(filepath)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, img_size)  # Resize for uniformity\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Paths to vehicle and non-vehicle folders\n",
    "vehicle_folder = 'data4/vehicles'\n",
    "no_vehicle_folder = \"data4/non-vehicles\"\n",
    "\n",
    "\n",
    "# Load vehicle and non-vehicle data\n",
    "vehicle_images, vehicle_labels = load_images_from_folder(vehicle_folder, 1)\n",
    "no_vehicle_images, no_vehicle_labels = load_images_from_folder(no_vehicle_folder, 0)\n",
    "\n",
    "# Combine and preprocess the dataset\n",
    "X = np.array(vehicle_images + no_vehicle_images) / 255.0  # Normalize pixel values to [0, 1]\n",
    "y = np.array(vehicle_labels + no_vehicle_labels)\n",
    "\n",
    "# Reshape data for CNN\n",
    "X = X.reshape(-1, 128, 128, 3)  # Adjust input dimensions\n",
    "y = to_categorical(y, num_classes=2)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Option 1: CNN from Scratch\n",
    "def cnn_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu', kernel_regularizer='l2'),  # L2 regularization\n",
    "        Dropout(0.5),  # Dropout for regularization\n",
    "        Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Option 2: Transfer Learning with MobileNetV2\n",
    "def mobilenet_model():\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False  # Freeze pre-trained layers\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Choose your model\n",
    "use_transfer_learning = True  # Set to False for CNN from scratch\n",
    "if use_transfer_learning:\n",
    "    model = mobilenet_model()\n",
    "    print(\"Using MobileNetV2 for Transfer Learning...\")\n",
    "else:\n",
    "    model = cnn_model()\n",
    "    print(\"Using Custom CNN from Scratch...\")\n",
    "\n",
    "# Early stopping to avoid overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"vehicle_detection_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b8fe804-9948-4f5b-84e7-7ec604f709d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step\n",
      "Prediction for data4/full.png: Vehicle\n"
     ]
    }
   ],
   "source": [
    "# Function to predict a new image\n",
    "def predict_vehicle(image_path, model):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (128, 128)) / 255.0  # Resize and normalize\n",
    "    img = img.reshape(1, 128, 128, 3)  # Add batch dimension\n",
    "    prediction = model.predict(img)\n",
    "    return \"Vehicle\" if np.argmax(prediction) == 1 else \"No Vehicle\"\n",
    "\n",
    "# Example prediction with a new image\n",
    "test_image_path = \"data4/full.png\"\n",
    "print(f\"Prediction for {test_image_path}: {predict_vehicle(test_image_path, model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ecb81b51-0356-49c3-9e91-6044b428deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(filepath, img_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocesses an image:\n",
    "    1. Reads and converts to grayscale.\n",
    "    2. Resizes to img_size.\n",
    "    3. Applies histogram equalization.\n",
    "    4. Applies Gaussian blur.\n",
    "    5. Normalizes pixel values to [0, 1].\n",
    "    \"\"\"\n",
    "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "    img = cv2.resize(img, img_size)  # Resize\n",
    "    img = cv2.equalizeHist(img)  # Histogram equalization\n",
    "    img = cv2.GaussianBlur(img, (3, 3), 0)  # Gaussian Blur\n",
    "    img = img / 255.0  # Normalize pixel values\n",
    "    return img\n",
    "\n",
    "# Function to load and preprocess images from a folder\n",
    "def load_images_from_folder(folder, label, img_size=(64, 64)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if filepath.endswith(('.png')):\n",
    "            img = preprocess_image(filepath, img_size)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Paths to vehicle and non-vehicle folders\n",
    "vehicle_folder = 'data4/vehicles'\n",
    "no_vehicle_folder = \"data4/non-vehicles\"\n",
    "\n",
    "# Load vehicle and non-vehicle data\n",
    "vehicle_images, vehicle_labels = load_images_from_folder(vehicle_folder, 1)\n",
    "no_vehicle_images, no_vehicle_labels = load_images_from_folder(no_vehicle_folder, 0)\n",
    "\n",
    "# Combine the data\n",
    "X = np.array(vehicle_images + no_vehicle_images)\n",
    "y = np.array(vehicle_labels + no_vehicle_labels)\n",
    "\n",
    "# Extract HOG features\n",
    "def extract_hog_features(images):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        feature = hog(img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                      cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
    "        features.append(feature)\n",
    "    return np.array(features)\n",
    "\n",
    "# Extract HOG features for the entire dataset\n",
    "X_hog = extract_hog_features(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hog, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143666a-e0ed-47db-b55d-258abfb0327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM model\n",
    "svm = LinearSVC(max_iter=10000)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = svm.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"No Vehicle\", \"Vehicle\"]))\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(svm, \"svm_vehicle_detection.pkl\")\n",
    "\n",
    "# Function to predict a single image\n",
    "def predict_vehicle(image_path, svm, img_size=(64, 64)):\n",
    "    img = preprocess_image(image_path, img_size)\n",
    "    hog_features = hog(img, orientations=9, pixels_per_cell=(8, 8), \n",
    "                       cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
    "    prediction = svm.predict([hog_features])\n",
    "    return \"Vehicle\" if prediction[0] == 1 else \"No Vehicle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709d0db-d1c0-4d22-9236-fb48e7dfabf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4bb6cac2-0a5e-4e9f-ae6a-36f07ba2595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for data4/full.png: No Vehicle\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "test_image_path = \"data4/full.png\"\n",
    "print(f\"Prediction for {test_image_path}: {predict_vehicle(test_image_path, svm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff371083-4a55-4452-b5e4-ea1981198c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 00:09:02.211 Python[80512:4979407] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained deep learning model\n",
    "model = load_model(\"vehicle_detection_model.keras\")  # Replace with your model file\n",
    "\n",
    "def sliding_window(image, step_size, window_size):\n",
    "    \"\"\"\n",
    "    Slide a window across the image.\n",
    "    :param image: Input image.\n",
    "    :param step_size: Number of pixels to move the window.\n",
    "    :param window_size: Tuple (width, height) for the window size.\n",
    "    :return: Generator yielding (x, y, window).\n",
    "    \"\"\"\n",
    "    for y in range(0, image.shape[0] - window_size[1], step_size):\n",
    "        for x in range(0, image.shape[1] - window_size[0], step_size):\n",
    "            yield (x, y, image[y:y + window_size[1], x:x + window_size[0]])\n",
    "\n",
    "def predict_and_draw_boxes(image_path, model, window_size=(64, 64), step_size=32, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect vehicles in an image and draw bounding boxes.\n",
    "    :param image_path: Path to input image.\n",
    "    :param model: Trained deep learning model.\n",
    "    :param window_size: Size of sliding window.\n",
    "    :param step_size: Step size for sliding window.\n",
    "    :param threshold: Prediction confidence threshold.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (512, 512))  # Resize for faster processing\n",
    "    orig_image = image.copy()\n",
    "    image = image / 255.0  # Normalize pixel values\n",
    "    \n",
    "    # List to store bounding boxes\n",
    "    boxes = []\n",
    "    \n",
    "    # Sliding window detection\n",
    "    for (x, y, window) in sliding_window(image, step_size, window_size):\n",
    "        if window.shape[0] != window_size[1] or window.shape[1] != window_size[0]:\n",
    "            continue  # Ignore windows that are not the correct size\n",
    "        \n",
    "        # Preprocess window\n",
    "        window = window.reshape(1, window_size[1], window_size[0], 3)  # Reshape for the model\n",
    "        \n",
    "        # Predict using the model\n",
    "        prediction = model.predict(window, verbose=0)\n",
    "        confidence = prediction[0][1]  # Confidence for 'Vehicle' class\n",
    "        \n",
    "        # If confidence exceeds threshold, save the bounding box\n",
    "        if confidence > threshold:\n",
    "            boxes.append((x, y, x + window_size[0], y + window_size[1]))\n",
    "    \n",
    "    # Draw bounding boxes on the original image\n",
    "    for (x1, y1, x2, y2) in boxes:\n",
    "        cv2.rectangle(orig_image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green rectangle\n",
    "    \n",
    "    # Display the image with bounding boxes\n",
    "    cv2.imshow(\"Vehicle Detection\", orig_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Optionally save the image with bounding boxes\n",
    "    cv2.imwrite(\"vehicle_detection_output.jpg\", orig_image)\n",
    "    print(\"Output image saved as 'data4/output.png'.\")\n",
    "\n",
    "# Path to the test image\n",
    "test_image_path = \"data4/carfull.png\"  # Replace with your test image path\n",
    "\n",
    "# Run vehicle detection\n",
    "# Run vehicle detection with correct window size\n",
    "predict_and_draw_boxes(test_image_path, model, window_size=(128, 128), step_size=32, threshold=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338cefd-8f16-47d6-bd12-426875df0ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 00:09:28.875 Python[80518:4984002] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained deep learning model\n",
    "model = load_model(\"vehicle_detection_model.keras\")  # Replace with your model file path\n",
    "\n",
    "def predict_whole_image(image_path, model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect vehicles using a single bounding box assumption.\n",
    "    :param image_path: Path to the input image.\n",
    "    :param model: Trained deep learning model.\n",
    "    :param threshold: Confidence threshold for vehicle detection.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path)\n",
    "    orig_image = image.copy()\n",
    "    \n",
    "    # Resize to match model input size\n",
    "    input_size = (128, 128)\n",
    "    resized_image = cv2.resize(image, input_size)\n",
    "    resized_image = resized_image / 255.0  # Normalize pixel values\n",
    "    resized_image = resized_image.reshape(1, input_size[0], input_size[1], 3)  # Add batch dimension\n",
    "    \n",
    "    # Predict using the model\n",
    "    prediction = model.predict(resized_image, verbose=0)\n",
    "    confidence = prediction[0][1]  # Confidence score for 'Vehicle' class\n",
    "    \n",
    "    # Draw a single bounding box if confidence exceeds threshold\n",
    "    if confidence > threshold:\n",
    "        height, width = orig_image.shape[:2]\n",
    "        x1, y1 = int(width * 0.2), int(height * 0.2)  # Adjust the box size as needed\n",
    "        x2, y2 = int(width * 0.8), int(height * 0.8)\n",
    "        \n",
    "        # Draw a green rectangle around the car\n",
    "        cv2.rectangle(orig_image, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        label = f\"Vehicle ({confidence*100:.2f}%)\"\n",
    "        cv2.putText(orig_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the output image\n",
    "    cv2.imshow(\"Detected Vehicle\", orig_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the output image\n",
    "    cv2.imwrite(\"vehicle_detection_single_box.jpg\", orig_image)\n",
    "    print(\"Output image saved as 'vehicle_detection_single_box.jpg'.\")\n",
    "\n",
    "# Path to the test image\n",
    "test_image_path = \"data4/carfull2.png\"  # Replace with your test image path\n",
    "\n",
    "\n",
    "# Run detection\n",
    "predict_whole_image(test_image_path, model, threshold=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6229c92-f5d0-4e77-9d9b-b9b9348304ab",
   "metadata": {},
   "source": [
    "# Using CNN boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c729f708-5067-4f15-85b9-31bd506431e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting imutils\n",
      "  Downloading imutils-0.5.4.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25837 sha256=512ca747bf094d3a865455306495caf4a97e1644b064ac2eb20c4ebc33b3eba7\n",
      "  Stored in directory: /Users/yashdode/Library/Caches/pip/wheels/5b/76/96/ad0c321506837bef578cf3008df3916c23018435a355d9f6b1\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059bfa71-a226-483c-9ac6-4699252f16dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 00:17:39.947 Python[80599:5010670] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "# Load the trained deep learning model\n",
    "model = load_model(\"vehicle_detection_model.keras\")  # Replace with your model file path\n",
    "\n",
    "def sliding_window(image, step_size, window_size):\n",
    "    \"\"\"\n",
    "    Slide a window across the image.\n",
    "    :param image: Input image.\n",
    "    :param step_size: Number of pixels to move the window.\n",
    "    :param window_size: Tuple (width, height) for the window size.\n",
    "    :return: Generator yielding (x, y, window).\n",
    "    \"\"\"\n",
    "    for y in range(0, image.shape[0] - window_size[1], step_size):\n",
    "        for x in range(0, image.shape[1] - window_size[0], step_size):\n",
    "            yield (x, y, image[y:y + window_size[1], x:x + window_size[0]])\n",
    "\n",
    "def multi_scale_detection(image_path, model, window_size=(128, 128), step_size=32, scales=[1.0, 1.5, 2.0], threshold=0.7):\n",
    "    \"\"\"\n",
    "    Detect vehicles in an image using sliding windows at multiple scales and apply NMS.\n",
    "    Ensures only the single most confident bounding box is drawn.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    orig_image = image.copy()\n",
    "    boxes = []  # List to store bounding boxes and confidence scores\n",
    "\n",
    "    # Multi-scale sliding window\n",
    "    for scale in scales:\n",
    "        resized_image = cv2.resize(image, None, fx=1/scale, fy=1/scale)\n",
    "        for (x, y, window) in sliding_window(resized_image, step_size, window_size):\n",
    "            if window.shape[0] != window_size[1] or window.shape[1] != window_size[0]:\n",
    "                continue  # Skip if the window is not the correct size\n",
    "            \n",
    "            # Preprocess the window for the model\n",
    "            window_resized = window / 255.0\n",
    "            window_resized = window_resized.reshape(1, window_size[1], window_size[0], 3)\n",
    "            \n",
    "            # Predict using the model\n",
    "            prediction = model.predict(window_resized, verbose=0)\n",
    "            confidence = prediction[0][1]  # Confidence for 'Vehicle' class\n",
    "            \n",
    "            # Save the bounding box if confidence exceeds threshold\n",
    "            if confidence > threshold:\n",
    "                x1 = int(x * scale)\n",
    "                y1 = int(y * scale)\n",
    "                x2 = int((x + window_size[0]) * scale)\n",
    "                y2 = int((y + window_size[1]) * scale)\n",
    "                boxes.append((x1, y1, x2, y2, confidence))\n",
    "\n",
    "    # Apply Non-Maximum Suppression (NMS)\n",
    "    rects = np.array([[x1, y1, x2, y2] for (x1, y1, x2, y2, _) in boxes])\n",
    "    scores = np.array([conf for (_, _, _, _, conf) in boxes])\n",
    "    selected_boxes = non_max_suppression(rects, probs=scores, overlapThresh=0.3)\n",
    "\n",
    "    # Select the single most confident bounding box\n",
    "    if len(selected_boxes) > 0:\n",
    "        best_box = selected_boxes[0]  # First box after NMS (highest confidence)\n",
    "        x1, y1, x2, y2 = best_box\n",
    "        cv2.rectangle(orig_image, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        cv2.putText(orig_image, \"Vehicle\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the output image\n",
    "    cv2.imshow(\"Vehicle Detection\", orig_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the output image\n",
    "    cv2.imwrite(\"vehicle_detection_single_box.jpg\", orig_image)\n",
    "    print(\"Output image saved as 'vehicle_detection_single_box.jpg'.\")\n",
    "\n",
    "# Path to the test image\n",
    "test_image_path = \"data4/carfull3.png\"  # Replace with your test image path\n",
    "\n",
    "# Run vehicle detection\n",
    "multi_scale_detection(test_image_path, model, window_size=(128, 128), step_size=64, scales=[1.0, 1.5, 2.0], threshold=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed70850-e797-4a8e-baec-0b75f4be7f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 00:28:25.467 Python[80709:5065311] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "# Load the trained deep learning model\n",
    "model = load_model(\"vehicle_detection_model.keras\")  # Replace with your model file path\n",
    "\n",
    "def sliding_window(image, step_size, window_size):\n",
    "    \"\"\"\n",
    "    Slide a rectangular window across the image.\n",
    "    :param image: Input image.\n",
    "    :param step_size: Number of pixels to move the window.\n",
    "    :param window_size: Tuple (width, height) for the window size.\n",
    "    :return: Generator yielding (x, y, window).\n",
    "    \"\"\"\n",
    "    for y in range(0, image.shape[0] - window_size[1], step_size):\n",
    "        for x in range(0, image.shape[1] - window_size[0], step_size):\n",
    "            yield (x, y, window_size[0], window_size[1], image[y:y + window_size[1], x:x + window_size[0]])\n",
    "\n",
    "def multi_scale_detection(image_path, model, window_sizes=[(192, 128), (256, 128)], step_size=64, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Detect vehicles in an image using sliding rectangular windows and apply NMS.\n",
    "    Ensures only the single most confident bounding box is drawn.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    orig_image = image.copy()\n",
    "    boxes = []  # List to store bounding boxes and confidence scores\n",
    "\n",
    "    # Sliding windows at multiple sizes\n",
    "    for window_size in window_sizes:\n",
    "        for (x, y, w, h, window) in sliding_window(image, step_size, window_size):\n",
    "            if window.shape[0] != h or window.shape[1] != w:\n",
    "                continue  # Skip invalid windows\n",
    "            \n",
    "            # Preprocess the window for the model\n",
    "            window_resized = cv2.resize(window, (128, 128)) / 255.0\n",
    "            window_resized = window_resized.reshape(1, 128, 128, 3)\n",
    "            \n",
    "            # Predict using the model\n",
    "            prediction = model.predict(window_resized, verbose=0)\n",
    "            confidence = prediction[0][1]  # Confidence for 'Vehicle' class\n",
    "            \n",
    "            # Save the bounding box if confidence exceeds threshold\n",
    "            if confidence > threshold:\n",
    "                x1, y1, x2, y2 = x, y, x + w, y + h\n",
    "                boxes.append((x1, y1, x2, y2, confidence))\n",
    "\n",
    "    # Apply Non-Maximum Suppression (NMS)\n",
    "    rects = np.array([[x1, y1, x2, y2] for (x1, y1, x2, y2, _) in boxes])\n",
    "    scores = np.array([conf for (_, _, _, _, conf) in boxes])\n",
    "    selected_boxes = non_max_suppression(rects, probs=scores, overlapThresh=0.3)\n",
    "\n",
    "    # Select the single most confident bounding box\n",
    "    if len(selected_boxes) > 0:\n",
    "        # Draw only the box with the highest confidence\n",
    "        best_box_index = np.argmax(scores)\n",
    "        x1, y1, x2, y2 = rects[best_box_index]\n",
    "        cv2.rectangle(orig_image, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "        label = \"Vehicle\"\n",
    "        cv2.putText(orig_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the output image\n",
    "    cv2.imshow(\"Refined Vehicle Detection\", orig_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the output image\n",
    "    cv2.imwrite(\"refined_vehicle_single_box.jpg\", orig_image)\n",
    "    print(\"Output image saved as 'refined_vehicle_single_box.jpg'.\")\n",
    "\n",
    "# Path to the test image\n",
    "test_image_path = \"data4/carfull3.png\"  # Replace with your test image path\n",
    "\n",
    "# Run refined vehicle detection\n",
    "multi_scale_detection(test_image_path, model, window_sizes=[(192, 128), (256, 128)], step_size=64, threshold=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2df2ac6-c197-4405-8474-9bf098c139d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.8.3)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: numpy in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision matplotlib opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e447a29-a025-4ca7-bc53-2b38b68bf21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: yolov5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (7.0.13)\n",
      "Requirement already satisfied: gitpython>=3.1.30 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (3.1.43)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from yolov5) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (4.7.0.72)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (10.2.0)\n",
      "Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (5.9.8)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from yolov5) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (1.12.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (0.17.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from yolov5) (4.66.5)\n",
      "Requirement already satisfied: ultralytics>=8.0.100 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (8.1.40)\n",
      "Requirement already satisfied: tensorboard>=2.4.1 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from yolov5) (2.18.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from yolov5) (2.2.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (0.13.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from yolov5) (74.0.0)\n",
      "Requirement already satisfied: fire in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (0.6.0)\n",
      "Requirement already satisfied: boto3>=1.19.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (1.34.74)\n",
      "Requirement already satisfied: sahi>=0.11.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (0.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (0.21.4)\n",
      "Requirement already satisfied: roboflow>=0.2.29 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from yolov5) (1.1.26)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.74 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from boto3>=1.19.1->yolov5) (1.34.74)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from boto3>=1.19.1->yolov5) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from boto3>=1.19.1->yolov5) (0.10.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gitpython>=3.1.30->yolov5) (4.0.11)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.12.0->yolov5) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.12.0->yolov5) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from huggingface-hub>=0.12.0->yolov5) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from huggingface-hub>=0.12.0->yolov5) (24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib>=3.3->yolov5) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from matplotlib>=3.3->yolov5) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from pandas>=1.1.4->yolov5) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from pandas>=1.1.4->yolov5) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from requests>=2.23.0->yolov5) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from requests>=2.23.0->yolov5) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from requests>=2.23.0->yolov5) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from requests>=2.23.0->yolov5) (2023.7.22)\n",
      "Requirement already satisfied: chardet==4.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from roboflow>=0.2.29->yolov5) (4.0.0)\n",
      "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from roboflow>=0.2.29->yolov5) (4.8.0.74)\n",
      "Requirement already satisfied: python-dotenv in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from roboflow>=0.2.29->yolov5) (1.0.1)\n",
      "Requirement already satisfied: six in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from roboflow>=0.2.29->yolov5) (1.16.0)\n",
      "Requirement already satisfied: requests-toolbelt in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from roboflow>=0.2.29->yolov5) (1.0.0)\n",
      "Requirement already satisfied: python-magic in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from roboflow>=0.2.29->yolov5) (0.4.27)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sahi>=0.11.10->yolov5) (2.0.3)\n",
      "Requirement already satisfied: pybboxes==0.1.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sahi>=0.11.10->yolov5) (0.1.6)\n",
      "Requirement already satisfied: terminaltables in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sahi>=0.11.10->yolov5) (3.1.10)\n",
      "Requirement already satisfied: click in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from sahi>=0.11.10->yolov5) (8.1.7)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from tensorboard>=2.4.1->yolov5) (1.66.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from tensorboard>=2.4.1->yolov5) (5.27.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard>=2.4.1->yolov5) (3.0.1)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.7.0->yolov5) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.7.0->yolov5) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from torch>=1.7.0->yolov5) (3.1.4)\n",
      "Requirement already satisfied: py-cpuinfo in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from ultralytics>=8.0.100->yolov5) (9.0.0)\n",
      "Requirement already satisfied: termcolor in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fire->yolov5) (2.4.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->yolov5) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/yashdode/Library/Python/3.12/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard>=2.4.1->yolov5) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch>=1.7.0->yolov5) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yolov5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2b2637-e7fe-4c06-9a6a-8cad95655d5a",
   "metadata": {},
   "source": [
    "# YOLO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e155b33-ee93-4400-93a9-565f106f52d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/yashdode/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2024-4-1 Python-3.12.0 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n",
      "2024-12-16 00:29:06.796 Python[80723:5071172] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLOv5 Model (pre-trained on COCO dataset)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # 'yolov5s' is the small model; you can use 'yolov5m' or 'yolov5l' for better accuracy\n",
    "\n",
    "def detect_vehicles_yolo(image_path, save_output=True):\n",
    "    \"\"\"\n",
    "    Detect vehicles in an image using YOLOv5.\n",
    "    :param image_path: Path to the input image.\n",
    "    :param save_output: Whether to save the output image.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(\"Error: Image not found.\")\n",
    "        return\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Perform inference using YOLO\n",
    "    results = model(image)\n",
    "\n",
    "    # Filter results to include only 'car', 'truck', or 'bus' classes\n",
    "    filtered_results = results.pandas().xyxy[0]  # Bounding boxes in pandas DataFrame format\n",
    "    vehicle_classes = ['car', 'truck', 'bus', 'motorbike']\n",
    "    filtered_results = filtered_results[filtered_results['name'].isin(vehicle_classes)]\n",
    "\n",
    "    # Draw bounding boxes on the image\n",
    "    for _, row in filtered_results.iterrows():\n",
    "        x1, y1, x2, y2 = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])\n",
    "        label = f\"{row['name']} {row['confidence']:.2f}\"\n",
    "        color = (0, 255, 0)  # Green bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    # Display the output image\n",
    "    cv2.imshow(\"YOLO Vehicle Detection\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the output image\n",
    "    if save_output:\n",
    "        output_path = \"yolo_vehicle_detection.jpg\"\n",
    "        cv2.imwrite(output_path, image)\n",
    "        print(f\"Output image saved as {output_path}.\")\n",
    "\n",
    "# Path to your test image\n",
    "image_path = \"data4/carfull3.png\"  # Replace with your image path\n",
    "\n",
    "# Run vehicle detection\n",
    "detect_vehicles_yolo(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e2327-e003-4f7b-b41d-575557c647c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
